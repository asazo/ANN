{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importar Librerías\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.16065049,  0.82730935]), 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejercicio 1.1\n",
    "def generate_data(n):\n",
    "    \n",
    "    # Lista para guardar datos etiquetados\n",
    "    output = list()\n",
    "    \n",
    "    # Generación de n tuplas aleatorias\n",
    "    input = 2 * np.random.random_sample((n,2)) - 1\n",
    "    \n",
    "    # Asignación datos dependiendo del cuadrante\n",
    "    for i in input:\n",
    "        # Cuadrante 1\n",
    "        if i[0] > 0 and i[1] > 0:\n",
    "            output.append(0)\n",
    "        # Cuadrante 2\n",
    "        elif i[0] < 0 and i[1] > 0: \n",
    "            output.append(1)\n",
    "        # Cuadrante 3\n",
    "        elif i[0] < 0 and i[1] < 0: \n",
    "            output.append(0)\n",
    "        # Cuadrante 4\n",
    "        elif i[0] > 0 and i[1] < 0: \n",
    "            output.append(1)\n",
    "    return input, output\n",
    "\n",
    "(x_training, y_training) = generate_data(1000)\n",
    "(x_test, y_test) = generate_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurona inicializada\n",
      "Neurona entrenada\n",
      "1000/1000 [==============================] - 0s\n",
      "0.249804571271\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 1.2\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "# Creación de una neurona\n",
    "model = Sequential()\n",
    "\n",
    "# Dimensión input = 1, Dimensión output = 2, función de activación es Relu\n",
    "model.add(Dense(output_dim=1, input_dim=2, init=\"normal\"))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "print \"Neurona inicializada\"\n",
    "\n",
    "# Entrenar a la neurona\n",
    "model.fit(x_training, y_training, nb_epoch=1000,verbose=0)\n",
    "\n",
    "print \"Neurona entrenada\"\n",
    "\n",
    "# Evaluar la neurona\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=1000)\n",
    "print loss_and_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4691\n",
      "0.5263\n",
      "0.4879\n",
      "0.5075\n"
     ]
    }
   ],
   "source": [
    "print round(model.predict(np.array([-1,-1]).reshape(1,2))[0][0],4)\n",
    "print round(model.predict(np.array([1,1]).reshape(1,2))[0][0],4)\n",
    "print round(model.predict(np.array([-1,1]).reshape(1,2))[0][0],4)\n",
    "print round(model.predict(np.array([1,-1]).reshape(1,2))[0][0],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurona inicializada\n",
      "Neurona entrenada\n",
      "1000/1000 [==============================] - 0s\n",
      "0.0075051295571\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 1.3\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "# Creación de una neurona\n",
    "xor = Sequential()\n",
    "\n",
    "# Dimensión input = 1, Dimensión output = 2, función de activación es Relu\n",
    "xor.add(Dense(8, input_dim = 2, activation = \"relu\"))\n",
    "xor.add(Dense(1, activation = \"sigmoid\"))\n",
    "xor.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "print \"Neurona inicializada\"\n",
    "\n",
    "# Entrenar a la neurona\n",
    "xor.fit(x_training, y_training, nb_epoch=1000, verbose=0)\n",
    "\n",
    "print \"Neurona entrenada\"\n",
    "\n",
    "# Evaluar la neurona\n",
    "evaluacion = xor.evaluate(x_test, y_test, batch_size=1000)\n",
    "print evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print round(xor.predict(np.array([-1,-1]).reshape(1,2))[0][0],4)\n",
    "print round(xor.predict(np.array([1,1]).reshape(1,2))[0][0],4)\n",
    "print round(xor.predict(np.array([-1,1]).reshape(1,2))[0][0],4)\n",
    "print round(xor.predict(np.array([1,-1]).reshape(1,2))[0][0],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'http://mldata.org/repository/data/download/csv/regression-datasets-housing/'\n",
    "df = pd.read_csv(url, sep=',',header=None, names=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX',\n",
    "'RM', 'AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV'])\n",
    "from sklearn.cross_validation import train_test_split\n",
    "df_train, df_test = train_test_split(df,test_size=0.25, random_state=0)\n",
    "# Explicar que hacen estas lineas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null int64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null int64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null int64\n",
      "TAX        506 non-null int64\n",
      "PTRATIO    506 non-null int64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "MEDV       506 non-null float64\n",
      "dtypes: float64(9), int64(5)\n",
      "memory usage: 59.3 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.347826</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.083004</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.310593</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.280574</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677082</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.347826   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.310593    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677082   12.000000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.083004  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.280574   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.000000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.000000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.000000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.000000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Normalización de datos. Este procedimiento es necesario para (completalo andrea)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "y_train_scaled = X_train_scaled.pop('MEDV')\n",
    "y_test_scaled = X_test_scaled.pop('MEDV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) oli $:3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.callbacks.History object at 0x7f18551082d0>\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=X_train_scaled.shape[1], init='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, init='uniform'))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "sgd = SGD(lr=0.2)\n",
    "model.compile(optimizer='sgd',loss='mean_squared_error')\n",
    "\n",
    "hist = model.fit(X_train_scaled.as_matrix(), y_train_scaled.as_matrix(), nb_epoch=300, verbose=0, validation_data=(X_test_scaled.as_matrix(), y_test_scaled.as_matrix()))\n",
    "print hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEKCAYAAAD5MJl4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEalC4SAiIlX8HzCsD3S9yJSHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1854c5ec10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "epochs = np.arange(300)\n",
    "plt.plot(epochs, hist.history['loss'], 'b-')\n",
    "plt.xlabel(\"# epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Variar función de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(200, input_dim=X_train_scaled.shape[1], init='uniform'))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(1, init='uniform'))\n",
    "model2.add(Activation('linear'))\n",
    "\n",
    "sgd = SGD(lr=0.2)\n",
    "model2.compile(optimizer='sgd',loss='mean_squared_error')\n",
    "\n",
    "hist2 = model2.fit(X_train_scaled.as_matrix(), y_train_scaled.as_matrix(), nb_epoch=300, verbose=0, validation_data=(X_test_scaled.as_matrix(), y_test_scaled.as_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEZCAYAAAB8culNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f184fee2890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "epochs = np.arange(300)\n",
    "plt.plot(epochs, hist2.history['loss'], 'r-')\n",
    "plt.title(u\"Función de activación ReLu\")\n",
    "plt.xlabel(\"# epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Variar learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_lr = 20\n",
    "lear_rate = np.linspace(0,1,n_lr)\n",
    "# La andrea lo va a hacer Firma:andrea 11:41\n",
    "# Modularizar modelo de d) para poder cambiar la función de activación y learning rate\n",
    "# Plotear fuera del for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "341/341 [==============================] - 0s - loss: 0.9639     \n",
      "Epoch 2/300\n",
      "341/341 [==============================] - 0s - loss: 0.8661     \n",
      "Epoch 3/300\n",
      "341/341 [==============================] - 0s - loss: 0.7756     \n",
      "Epoch 4/300\n",
      "341/341 [==============================] - 0s - loss: 0.6866     \n",
      "Epoch 5/300\n",
      "341/341 [==============================] - 0s - loss: 0.6029     \n",
      "Epoch 6/300\n",
      "341/341 [==============================] - 0s - loss: 0.5270     \n",
      "Epoch 7/300\n",
      "341/341 [==============================] - 0s - loss: 0.4636     \n",
      "Epoch 8/300\n",
      "341/341 [==============================] - 0s - loss: 0.4108     \n",
      "Epoch 9/300\n",
      "341/341 [==============================] - 0s - loss: 0.3683     \n",
      "Epoch 10/300\n",
      "341/341 [==============================] - 0s - loss: 0.3325     \n",
      "Epoch 11/300\n",
      "341/341 [==============================] - 0s - loss: 0.3057     \n",
      "Epoch 12/300\n",
      "341/341 [==============================] - 0s - loss: 0.2817     \n",
      "Epoch 13/300\n",
      "341/341 [==============================] - 0s - loss: 0.2624     \n",
      "Epoch 14/300\n",
      "341/341 [==============================] - 0s - loss: 0.2473     \n",
      "Epoch 15/300\n",
      "341/341 [==============================] - 0s - loss: 0.2338     \n",
      "Epoch 16/300\n",
      "341/341 [==============================] - 0s - loss: 0.2231     \n",
      "Epoch 17/300\n",
      "341/341 [==============================] - 0s - loss: 0.2153     \n",
      "Epoch 18/300\n",
      "341/341 [==============================] - 0s - loss: 0.2083     \n",
      "Epoch 19/300\n",
      "341/341 [==============================] - 0s - loss: 0.2024     \n",
      "Epoch 20/300\n",
      "341/341 [==============================] - 0s - loss: 0.1977     \n",
      "Epoch 21/300\n",
      "341/341 [==============================] - 0s - loss: 0.1934     \n",
      "Epoch 22/300\n",
      "341/341 [==============================] - 0s - loss: 0.1904     \n",
      "Epoch 23/300\n",
      "341/341 [==============================] - 0s - loss: 0.1875     \n",
      "Epoch 24/300\n",
      "341/341 [==============================] - 0s - loss: 0.1840     \n",
      "Epoch 25/300\n",
      "341/341 [==============================] - 0s - loss: 0.1817     \n",
      "Epoch 26/300\n",
      "341/341 [==============================] - 0s - loss: 0.1792     \n",
      "Epoch 27/300\n",
      "341/341 [==============================] - 0s - loss: 0.1768     \n",
      "Epoch 28/300\n",
      "341/341 [==============================] - 0s - loss: 0.1748     \n",
      "Epoch 29/300\n",
      "341/341 [==============================] - 0s - loss: 0.1727     \n",
      "Epoch 30/300\n",
      "341/341 [==============================] - 0s - loss: 0.1709     \n",
      "Epoch 31/300\n",
      "341/341 [==============================] - 0s - loss: 0.1691     \n",
      "Epoch 32/300\n",
      "341/341 [==============================] - 0s - loss: 0.1677     \n",
      "Epoch 33/300\n",
      "341/341 [==============================] - 0s - loss: 0.1666     \n",
      "Epoch 34/300\n",
      "341/341 [==============================] - 0s - loss: 0.1642     \n",
      "Epoch 35/300\n",
      "341/341 [==============================] - 0s - loss: 0.1630     \n",
      "Epoch 36/300\n",
      "341/341 [==============================] - 0s - loss: 0.1623     \n",
      "Epoch 37/300\n",
      "341/341 [==============================] - 0s - loss: 0.1605     \n",
      "Epoch 38/300\n",
      "341/341 [==============================] - 0s - loss: 0.1591     \n",
      "Epoch 39/300\n",
      "341/341 [==============================] - 0s - loss: 0.1583     \n",
      "Epoch 40/300\n",
      "341/341 [==============================] - 0s - loss: 0.1566     \n",
      "Epoch 41/300\n",
      "341/341 [==============================] - 0s - loss: 0.1555     \n",
      "Epoch 42/300\n",
      "341/341 [==============================] - 0s - loss: 0.1544     \n",
      "Epoch 43/300\n",
      "341/341 [==============================] - 0s - loss: 0.1532     \n",
      "Epoch 44/300\n",
      "341/341 [==============================] - 0s - loss: 0.1520     \n",
      "Epoch 45/300\n",
      "341/341 [==============================] - 0s - loss: 0.1510     \n",
      "Epoch 46/300\n",
      "341/341 [==============================] - 0s - loss: 0.1504     \n",
      "Epoch 47/300\n",
      "341/341 [==============================] - 0s - loss: 0.1490     \n",
      "Epoch 48/300\n",
      "341/341 [==============================] - 0s - loss: 0.1483     \n",
      "Epoch 49/300\n",
      "341/341 [==============================] - 0s - loss: 0.1471     \n",
      "Epoch 50/300\n",
      "341/341 [==============================] - 0s - loss: 0.1461     \n",
      "Epoch 51/300\n",
      "341/341 [==============================] - 0s - loss: 0.1447     \n",
      "Epoch 52/300\n",
      "341/341 [==============================] - 0s - loss: 0.1441     \n",
      "Epoch 53/300\n",
      "341/341 [==============================] - 0s - loss: 0.1430     \n",
      "Epoch 54/300\n",
      "341/341 [==============================] - 0s - loss: 0.1425     \n",
      "Epoch 55/300\n",
      "341/341 [==============================] - 0s - loss: 0.1414     \n",
      "Epoch 56/300\n",
      "341/341 [==============================] - 0s - loss: 0.1410     \n",
      "Epoch 57/300\n",
      "341/341 [==============================] - 0s - loss: 0.1398     \n",
      "Epoch 58/300\n",
      "341/341 [==============================] - 0s - loss: 0.1390     \n",
      "Epoch 59/300\n",
      "341/341 [==============================] - 0s - loss: 0.1379     \n",
      "Epoch 60/300\n",
      "341/341 [==============================] - 0s - loss: 0.1376     \n",
      "Epoch 61/300\n",
      "341/341 [==============================] - 0s - loss: 0.1369     \n",
      "Epoch 62/300\n",
      "341/341 [==============================] - 0s - loss: 0.1362     \n",
      "Epoch 63/300\n",
      "341/341 [==============================] - 0s - loss: 0.1349     \n",
      "Epoch 64/300\n",
      "341/341 [==============================] - 0s - loss: 0.1345     \n",
      "Epoch 65/300\n",
      "341/341 [==============================] - 0s - loss: 0.1338     \n",
      "Epoch 66/300\n",
      "341/341 [==============================] - 0s - loss: 0.1329     \n",
      "Epoch 67/300\n",
      "341/341 [==============================] - 0s - loss: 0.1326     \n",
      "Epoch 68/300\n",
      "341/341 [==============================] - 0s - loss: 0.1319     \n",
      "Epoch 69/300\n",
      "341/341 [==============================] - 0s - loss: 0.1312     \n",
      "Epoch 70/300\n",
      "341/341 [==============================] - 0s - loss: 0.1303     \n",
      "Epoch 71/300\n",
      "341/341 [==============================] - 0s - loss: 0.1292     \n",
      "Epoch 72/300\n",
      "341/341 [==============================] - 0s - loss: 0.1290     \n",
      "Epoch 73/300\n",
      "341/341 [==============================] - 0s - loss: 0.1282     \n",
      "Epoch 74/300\n",
      "341/341 [==============================] - 0s - loss: 0.1276     \n",
      "Epoch 75/300\n",
      "341/341 [==============================] - 0s - loss: 0.1269     \n",
      "Epoch 76/300\n",
      "341/341 [==============================] - 0s - loss: 0.1267     \n",
      "Epoch 77/300\n",
      "341/341 [==============================] - 0s - loss: 0.1262     \n",
      "Epoch 78/300\n",
      "341/341 [==============================] - 0s - loss: 0.1254     \n",
      "Epoch 79/300\n",
      "341/341 [==============================] - 0s - loss: 0.1245     \n",
      "Epoch 80/300\n",
      "341/341 [==============================] - 0s - loss: 0.1245     \n",
      "Epoch 81/300\n",
      "341/341 [==============================] - 0s - loss: 0.1236     \n",
      "Epoch 82/300\n",
      "341/341 [==============================] - 0s - loss: 0.1230     \n",
      "Epoch 83/300\n",
      "341/341 [==============================] - 0s - loss: 0.1226     \n",
      "Epoch 84/300\n",
      "341/341 [==============================] - 0s - loss: 0.1221     \n",
      "Epoch 85/300\n",
      "341/341 [==============================] - 0s - loss: 0.1215     \n",
      "Epoch 86/300\n",
      "341/341 [==============================] - 0s - loss: 0.1209     \n",
      "Epoch 87/300\n",
      "341/341 [==============================] - 0s - loss: 0.1209     \n",
      "Epoch 88/300\n",
      "341/341 [==============================] - 0s - loss: 0.1203     \n",
      "Epoch 89/300\n",
      "341/341 [==============================] - 0s - loss: 0.1194     \n",
      "Epoch 90/300\n",
      "341/341 [==============================] - 0s - loss: 0.1182     \n",
      "Epoch 91/300\n",
      "341/341 [==============================] - 0s - loss: 0.1180     \n",
      "Epoch 92/300\n",
      "341/341 [==============================] - 0s - loss: 0.1183     \n",
      "Epoch 93/300\n",
      "341/341 [==============================] - 0s - loss: 0.1173     \n",
      "Epoch 94/300\n",
      "341/341 [==============================] - 0s - loss: 0.1169     \n",
      "Epoch 95/300\n",
      "341/341 [==============================] - 0s - loss: 0.1162     \n",
      "Epoch 96/300\n",
      "341/341 [==============================] - 0s - loss: 0.1162     \n",
      "Epoch 97/300\n",
      "341/341 [==============================] - 0s - loss: 0.1160     \n",
      "Epoch 98/300\n",
      "341/341 [==============================] - 0s - loss: 0.1158     \n",
      "Epoch 99/300\n",
      "341/341 [==============================] - 0s - loss: 0.1146     \n",
      "Epoch 100/300\n",
      "341/341 [==============================] - 0s - loss: 0.1141     \n",
      "Epoch 101/300\n",
      "341/341 [==============================] - 0s - loss: 0.1136     \n",
      "Epoch 102/300\n",
      "341/341 [==============================] - 0s - loss: 0.1133     \n",
      "Epoch 103/300\n",
      "341/341 [==============================] - 0s - loss: 0.1126     \n",
      "Epoch 104/300\n",
      "341/341 [==============================] - 0s - loss: 0.1125     \n",
      "Epoch 105/300\n",
      "341/341 [==============================] - 0s - loss: 0.1120     \n",
      "Epoch 106/300\n",
      "341/341 [==============================] - 0s - loss: 0.1116     \n",
      "Epoch 107/300\n",
      "341/341 [==============================] - 0s - loss: 0.1110     \n",
      "Epoch 108/300\n",
      "341/341 [==============================] - 0s - loss: 0.1107     \n",
      "Epoch 109/300\n",
      "341/341 [==============================] - 0s - loss: 0.1103     \n",
      "Epoch 110/300\n",
      "341/341 [==============================] - 0s - loss: 0.1106     \n",
      "Epoch 111/300\n",
      "341/341 [==============================] - 0s - loss: 0.1092     \n",
      "Epoch 112/300\n",
      "341/341 [==============================] - 0s - loss: 0.1091     \n",
      "Epoch 113/300\n",
      "341/341 [==============================] - 0s - loss: 0.1082     \n",
      "Epoch 114/300\n",
      "341/341 [==============================] - 0s - loss: 0.1083     \n",
      "Epoch 115/300\n",
      "341/341 [==============================] - 0s - loss: 0.1073     \n",
      "Epoch 116/300\n",
      "341/341 [==============================] - 0s - loss: 0.1071     \n",
      "Epoch 117/300\n",
      "341/341 [==============================] - 0s - loss: 0.1073     \n",
      "Epoch 118/300\n",
      "341/341 [==============================] - 0s - loss: 0.1064     \n",
      "Epoch 119/300\n",
      "341/341 [==============================] - 0s - loss: 0.1062     \n",
      "Epoch 120/300\n",
      "341/341 [==============================] - 0s - loss: 0.1057     \n",
      "Epoch 121/300\n",
      "341/341 [==============================] - 0s - loss: 0.1056     \n",
      "Epoch 122/300\n",
      "341/341 [==============================] - 0s - loss: 0.1051     \n",
      "Epoch 123/300\n",
      "341/341 [==============================] - 0s - loss: 0.1050     \n",
      "Epoch 124/300\n",
      "341/341 [==============================] - 0s - loss: 0.1039     \n",
      "Epoch 125/300\n",
      "341/341 [==============================] - 0s - loss: 0.1041     \n",
      "Epoch 126/300\n",
      "341/341 [==============================] - 0s - loss: 0.1033     \n",
      "Epoch 127/300\n",
      "341/341 [==============================] - 0s - loss: 0.1030     \n",
      "Epoch 128/300\n",
      "341/341 [==============================] - 0s - loss: 0.1033     \n",
      "Epoch 129/300\n",
      "341/341 [==============================] - 0s - loss: 0.1029     \n",
      "Epoch 130/300\n",
      "341/341 [==============================] - 0s - loss: 0.1033     \n",
      "Epoch 131/300\n",
      "341/341 [==============================] - 0s - loss: 0.1018     \n",
      "Epoch 132/300\n",
      "341/341 [==============================] - 0s - loss: 0.1016     \n",
      "Epoch 133/300\n",
      "341/341 [==============================] - 0s - loss: 0.1014     \n",
      "Epoch 134/300\n",
      "341/341 [==============================] - 0s - loss: 0.1011     \n",
      "Epoch 135/300\n",
      "341/341 [==============================] - 0s - loss: 0.1008     \n",
      "Epoch 136/300\n",
      "341/341 [==============================] - 0s - loss: 0.1004     \n",
      "Epoch 137/300\n",
      "341/341 [==============================] - 0s - loss: 0.1000     \n",
      "Epoch 138/300\n",
      "341/341 [==============================] - 0s - loss: 0.0997     \n",
      "Epoch 139/300\n",
      "341/341 [==============================] - 0s - loss: 0.0992     \n",
      "Epoch 140/300\n",
      "341/341 [==============================] - 0s - loss: 0.0996     \n",
      "Epoch 141/300\n",
      "341/341 [==============================] - 0s - loss: 0.0989     \n",
      "Epoch 142/300\n",
      "341/341 [==============================] - 0s - loss: 0.0990     \n",
      "Epoch 143/300\n",
      "341/341 [==============================] - 0s - loss: 0.0980     \n",
      "Epoch 144/300\n",
      "341/341 [==============================] - 0s - loss: 0.0983     \n",
      "Epoch 145/300\n",
      "341/341 [==============================] - 0s - loss: 0.0979     \n",
      "Epoch 146/300\n",
      "341/341 [==============================] - 0s - loss: 0.0975     \n",
      "Epoch 147/300\n",
      "341/341 [==============================] - 0s - loss: 0.0969     \n",
      "Epoch 148/300\n",
      "341/341 [==============================] - 0s - loss: 0.0970     \n",
      "Epoch 149/300\n",
      "341/341 [==============================] - 0s - loss: 0.0968     \n",
      "Epoch 150/300\n",
      "341/341 [==============================] - 0s - loss: 0.0967     \n",
      "Epoch 151/300\n",
      "341/341 [==============================] - 0s - loss: 0.0961     \n",
      "Epoch 152/300\n",
      "341/341 [==============================] - 0s - loss: 0.0959     \n",
      "Epoch 153/300\n",
      "341/341 [==============================] - 0s - loss: 0.0955     \n",
      "Epoch 154/300\n",
      "341/341 [==============================] - 0s - loss: 0.0954     \n",
      "Epoch 155/300\n",
      "341/341 [==============================] - 0s - loss: 0.0955     \n",
      "Epoch 156/300\n",
      "341/341 [==============================] - 0s - loss: 0.0949     \n",
      "Epoch 157/300\n",
      "341/341 [==============================] - 0s - loss: 0.0942     \n",
      "Epoch 158/300\n",
      "341/341 [==============================] - 0s - loss: 0.0946     \n",
      "Epoch 159/300\n",
      "341/341 [==============================] - 0s - loss: 0.0941     \n",
      "Epoch 160/300\n",
      "341/341 [==============================] - 0s - loss: 0.0939     \n",
      "Epoch 161/300\n",
      "341/341 [==============================] - 0s - loss: 0.0938     \n",
      "Epoch 162/300\n",
      "341/341 [==============================] - 0s - loss: 0.0932     \n",
      "Epoch 163/300\n",
      "341/341 [==============================] - 0s - loss: 0.0925     \n",
      "Epoch 164/300\n",
      "341/341 [==============================] - 0s - loss: 0.0927     \n",
      "Epoch 165/300\n",
      "341/341 [==============================] - 0s - loss: 0.0927     \n",
      "Epoch 166/300\n",
      "341/341 [==============================] - 0s - loss: 0.0926     \n",
      "Epoch 167/300\n",
      "341/341 [==============================] - 0s - loss: 0.0921     \n",
      "Epoch 168/300\n",
      "341/341 [==============================] - 0s - loss: 0.0923     \n",
      "Epoch 169/300\n",
      "341/341 [==============================] - 0s - loss: 0.0912     \n",
      "Epoch 170/300\n",
      "341/341 [==============================] - 0s - loss: 0.0916     \n",
      "Epoch 171/300\n",
      "341/341 [==============================] - 0s - loss: 0.0915     \n",
      "Epoch 172/300\n",
      "341/341 [==============================] - 0s - loss: 0.0908     \n",
      "Epoch 173/300\n",
      "341/341 [==============================] - 0s - loss: 0.0902     \n",
      "Epoch 174/300\n",
      "341/341 [==============================] - 0s - loss: 0.0907     \n",
      "Epoch 175/300\n",
      "341/341 [==============================] - 0s - loss: 0.0903     \n",
      "Epoch 176/300\n",
      "341/341 [==============================] - 0s - loss: 0.0898     \n",
      "Epoch 177/300\n",
      "341/341 [==============================] - 0s - loss: 0.0899     \n",
      "Epoch 178/300\n",
      "341/341 [==============================] - 0s - loss: 0.0896     \n",
      "Epoch 179/300\n",
      "341/341 [==============================] - 0s - loss: 0.0893     \n",
      "Epoch 180/300\n",
      "341/341 [==============================] - 0s - loss: 0.0893     \n",
      "Epoch 181/300\n",
      "341/341 [==============================] - 0s - loss: 0.0890     \n",
      "Epoch 182/300\n",
      "341/341 [==============================] - 0s - loss: 0.0887     \n",
      "Epoch 183/300\n",
      "341/341 [==============================] - 0s - loss: 0.0885     \n",
      "Epoch 184/300\n",
      "341/341 [==============================] - 0s - loss: 0.0883     \n",
      "Epoch 185/300\n",
      "341/341 [==============================] - 0s - loss: 0.0877     \n",
      "Epoch 186/300\n",
      "341/341 [==============================] - 0s - loss: 0.0879     \n",
      "Epoch 187/300\n",
      "341/341 [==============================] - 0s - loss: 0.0880     \n",
      "Epoch 188/300\n",
      "341/341 [==============================] - 0s - loss: 0.0877     \n",
      "Epoch 189/300\n",
      "341/341 [==============================] - 0s - loss: 0.0871     \n",
      "Epoch 190/300\n",
      "341/341 [==============================] - 0s - loss: 0.0870     \n",
      "Epoch 191/300\n",
      "341/341 [==============================] - 0s - loss: 0.0869     \n",
      "Epoch 192/300\n",
      "341/341 [==============================] - 0s - loss: 0.0869     \n",
      "Epoch 193/300\n",
      "341/341 [==============================] - 0s - loss: 0.0866     \n",
      "Epoch 194/300\n",
      "341/341 [==============================] - 0s - loss: 0.0865     \n",
      "Epoch 195/300\n",
      "341/341 [==============================] - 0s - loss: 0.0864     \n",
      "Epoch 196/300\n",
      "341/341 [==============================] - 0s - loss: 0.0861     \n",
      "Epoch 197/300\n",
      "341/341 [==============================] - 0s - loss: 0.0860     \n",
      "Epoch 198/300\n",
      "341/341 [==============================] - 0s - loss: 0.0854     \n",
      "Epoch 199/300\n",
      "341/341 [==============================] - 0s - loss: 0.0857     \n",
      "Epoch 200/300\n",
      "341/341 [==============================] - 0s - loss: 0.0851     \n",
      "Epoch 201/300\n",
      "341/341 [==============================] - 0s - loss: 0.0848     \n",
      "Epoch 202/300\n",
      "341/341 [==============================] - 0s - loss: 0.0854     \n",
      "Epoch 203/300\n",
      "341/341 [==============================] - 0s - loss: 0.0845     \n",
      "Epoch 204/300\n",
      "341/341 [==============================] - 0s - loss: 0.0844     \n",
      "Epoch 205/300\n",
      "341/341 [==============================] - 0s - loss: 0.0845     \n",
      "Epoch 206/300\n",
      "341/341 [==============================] - 0s - loss: 0.0840     \n",
      "Epoch 207/300\n",
      "341/341 [==============================] - 0s - loss: 0.0837     \n",
      "Epoch 208/300\n",
      "341/341 [==============================] - 0s - loss: 0.0836     \n",
      "Epoch 209/300\n",
      "341/341 [==============================] - 0s - loss: 0.0833     \n",
      "Epoch 210/300\n",
      "341/341 [==============================] - 0s - loss: 0.0834     \n",
      "Epoch 211/300\n",
      "341/341 [==============================] - 0s - loss: 0.0833     \n",
      "Epoch 212/300\n",
      "341/341 [==============================] - 0s - loss: 0.0834     \n",
      "Epoch 213/300\n",
      "341/341 [==============================] - 0s - loss: 0.0831     \n",
      "Epoch 214/300\n",
      "341/341 [==============================] - 0s - loss: 0.0828     \n",
      "Epoch 215/300\n",
      "341/341 [==============================] - 0s - loss: 0.0822     \n",
      "Epoch 216/300\n",
      "341/341 [==============================] - 0s - loss: 0.0827     \n",
      "Epoch 217/300\n",
      "341/341 [==============================] - 0s - loss: 0.0821     \n",
      "Epoch 218/300\n",
      "341/341 [==============================] - 0s - loss: 0.0821     \n",
      "Epoch 219/300\n",
      "341/341 [==============================] - 0s - loss: 0.0820     \n",
      "Epoch 220/300\n",
      "341/341 [==============================] - 0s - loss: 0.0819     \n",
      "Epoch 221/300\n",
      "341/341 [==============================] - 0s - loss: 0.0816     \n",
      "Epoch 222/300\n",
      "341/341 [==============================] - 0s - loss: 0.0820     \n",
      "Epoch 223/300\n",
      "341/341 [==============================] - 0s - loss: 0.0814     \n",
      "Epoch 224/300\n",
      "341/341 [==============================] - 0s - loss: 0.0813     \n",
      "Epoch 225/300\n",
      "341/341 [==============================] - 0s - loss: 0.0811     \n",
      "Epoch 226/300\n",
      "341/341 [==============================] - 0s - loss: 0.0810     \n",
      "Epoch 227/300\n",
      "341/341 [==============================] - 0s - loss: 0.0808     \n",
      "Epoch 228/300\n",
      "341/341 [==============================] - 0s - loss: 0.0809     \n",
      "Epoch 229/300\n",
      "341/341 [==============================] - 0s - loss: 0.0805     \n",
      "Epoch 230/300\n",
      "341/341 [==============================] - 0s - loss: 0.0801     \n",
      "Epoch 231/300\n",
      "341/341 [==============================] - 0s - loss: 0.0802     \n",
      "Epoch 232/300\n",
      "341/341 [==============================] - 0s - loss: 0.0801     \n",
      "Epoch 233/300\n",
      "341/341 [==============================] - 0s - loss: 0.0798     \n",
      "Epoch 234/300\n",
      "341/341 [==============================] - 0s - loss: 0.0798     \n",
      "Epoch 235/300\n",
      "341/341 [==============================] - 0s - loss: 0.0794     \n",
      "Epoch 236/300\n",
      "341/341 [==============================] - 0s - loss: 0.0795     \n",
      "Epoch 237/300\n",
      "341/341 [==============================] - 0s - loss: 0.0791     \n",
      "Epoch 238/300\n",
      "341/341 [==============================] - 0s - loss: 0.0792     \n",
      "Epoch 239/300\n",
      "341/341 [==============================] - 0s - loss: 0.0792     \n",
      "Epoch 240/300\n",
      "341/341 [==============================] - 0s - loss: 0.0789     \n",
      "Epoch 241/300\n",
      "341/341 [==============================] - 0s - loss: 0.0791     \n",
      "Epoch 242/300\n",
      "341/341 [==============================] - 0s - loss: 0.0785     \n",
      "Epoch 243/300\n",
      "341/341 [==============================] - 0s - loss: 0.0783     \n",
      "Epoch 244/300\n",
      "341/341 [==============================] - 0s - loss: 0.0784     \n",
      "Epoch 245/300\n",
      "341/341 [==============================] - 0s - loss: 0.0780     \n",
      "Epoch 246/300\n",
      "341/341 [==============================] - 0s - loss: 0.0778     \n",
      "Epoch 247/300\n",
      "341/341 [==============================] - 0s - loss: 0.0780     \n",
      "Epoch 248/300\n",
      "341/341 [==============================] - 0s - loss: 0.0777     \n",
      "Epoch 249/300\n",
      "341/341 [==============================] - 0s - loss: 0.0775     \n",
      "Epoch 250/300\n",
      "341/341 [==============================] - 0s - loss: 0.0770     \n",
      "Epoch 251/300\n",
      "341/341 [==============================] - 0s - loss: 0.0771     \n",
      "Epoch 252/300\n",
      "341/341 [==============================] - 0s - loss: 0.0772     \n",
      "Epoch 253/300\n",
      "341/341 [==============================] - 0s - loss: 0.0769     \n",
      "Epoch 254/300\n",
      "341/341 [==============================] - 0s - loss: 0.0771     \n",
      "Epoch 255/300\n",
      "341/341 [==============================] - 0s - loss: 0.0767     \n",
      "Epoch 256/300\n",
      "341/341 [==============================] - 0s - loss: 0.0762     \n",
      "Epoch 257/300\n",
      "341/341 [==============================] - 0s - loss: 0.0762     \n",
      "Epoch 258/300\n",
      "341/341 [==============================] - 0s - loss: 0.0762     \n",
      "Epoch 259/300\n",
      "341/341 [==============================] - 0s - loss: 0.0760     \n",
      "Epoch 260/300\n",
      "341/341 [==============================] - 0s - loss: 0.0759     \n",
      "Epoch 261/300\n",
      "341/341 [==============================] - 0s - loss: 0.0758     \n",
      "Epoch 262/300\n",
      "341/341 [==============================] - 0s - loss: 0.0756     \n",
      "Epoch 263/300\n",
      "341/341 [==============================] - 0s - loss: 0.0755     \n",
      "Epoch 264/300\n",
      "341/341 [==============================] - 0s - loss: 0.0755     \n",
      "Epoch 265/300\n",
      "341/341 [==============================] - 0s - loss: 0.0750     \n",
      "Epoch 266/300\n",
      "341/341 [==============================] - 0s - loss: 0.0750     \n",
      "Epoch 267/300\n",
      "341/341 [==============================] - 0s - loss: 0.0750     \n",
      "Epoch 268/300\n",
      "341/341 [==============================] - 0s - loss: 0.0748     \n",
      "Epoch 269/300\n",
      "341/341 [==============================] - 0s - loss: 0.0748     \n",
      "Epoch 270/300\n",
      "341/341 [==============================] - 0s - loss: 0.0745     \n",
      "Epoch 271/300\n",
      "341/341 [==============================] - 0s - loss: 0.0747     \n",
      "Epoch 272/300\n",
      "341/341 [==============================] - 0s - loss: 0.0740     \n",
      "Epoch 273/300\n",
      "341/341 [==============================] - 0s - loss: 0.0737     \n",
      "Epoch 274/300\n",
      "341/341 [==============================] - 0s - loss: 0.0736     \n",
      "Epoch 275/300\n",
      "341/341 [==============================] - 0s - loss: 0.0740     \n",
      "Epoch 276/300\n",
      "341/341 [==============================] - 0s - loss: 0.0741     \n",
      "Epoch 277/300\n",
      "341/341 [==============================] - 0s - loss: 0.0738     \n",
      "Epoch 278/300\n",
      "341/341 [==============================] - 0s - loss: 0.0733     \n",
      "Epoch 279/300\n",
      "341/341 [==============================] - 0s - loss: 0.0733     \n",
      "Epoch 280/300\n",
      "341/341 [==============================] - 0s - loss: 0.0731     \n",
      "Epoch 281/300\n",
      "341/341 [==============================] - 0s - loss: 0.0729     \n",
      "Epoch 282/300\n",
      "341/341 [==============================] - 0s - loss: 0.0729     \n",
      "Epoch 283/300\n",
      "341/341 [==============================] - 0s - loss: 0.0726     \n",
      "Epoch 284/300\n",
      "341/341 [==============================] - 0s - loss: 0.0728     \n",
      "Epoch 285/300\n",
      "341/341 [==============================] - 0s - loss: 0.0728     \n",
      "Epoch 286/300\n",
      "341/341 [==============================] - 0s - loss: 0.0724     \n",
      "Epoch 287/300\n",
      "341/341 [==============================] - 0s - loss: 0.0721     \n",
      "Epoch 288/300\n",
      "341/341 [==============================] - 0s - loss: 0.0723     \n",
      "Epoch 289/300\n",
      "341/341 [==============================] - 0s - loss: 0.0719     \n",
      "Epoch 290/300\n",
      "341/341 [==============================] - 0s - loss: 0.0719     \n",
      "Epoch 291/300\n",
      "341/341 [==============================] - 0s - loss: 0.0714     \n",
      "Epoch 292/300\n",
      "341/341 [==============================] - 0s - loss: 0.0714     \n",
      "Epoch 293/300\n",
      "341/341 [==============================] - 0s - loss: 0.0713     \n",
      "Epoch 294/300\n",
      "341/341 [==============================] - 0s - loss: 0.0712     \n",
      "Epoch 295/300\n",
      "341/341 [==============================] - 0s - loss: 0.0716     \n",
      "Epoch 296/300\n",
      "341/341 [==============================] - 0s - loss: 0.0704     \n",
      "Epoch 297/300\n",
      "341/341 [==============================] - 0s - loss: 0.0707     \n",
      "Epoch 298/300\n",
      "341/341 [==============================] - 0s - loss: 0.0703     \n",
      "Epoch 299/300\n",
      "341/341 [==============================] - 0s - loss: 0.0707     \n",
      "Epoch 300/300\n",
      "341/341 [==============================] - 0s - loss: 0.0707     \n",
      "32/38 [========================>.....] - ETA: 0sEpoch 1/300\n",
      "341/341 [==============================] - 0s - loss: 0.9538     \n",
      "Epoch 2/300\n",
      "341/341 [==============================] - 0s - loss: 0.8481     \n",
      "Epoch 3/300\n",
      "341/341 [==============================] - 0s - loss: 0.7513     \n",
      "Epoch 4/300\n",
      "341/341 [==============================] - 0s - loss: 0.6585     \n",
      "Epoch 5/300\n",
      "341/341 [==============================] - 0s - loss: 0.5730     \n",
      "Epoch 6/300\n",
      "341/341 [==============================] - 0s - loss: 0.4973     \n",
      "Epoch 7/300\n",
      "341/341 [==============================] - 0s - loss: 0.4372     \n",
      "Epoch 8/300\n",
      "341/341 [==============================] - 0s - loss: 0.3903     \n",
      "Epoch 9/300\n",
      "341/341 [==============================] - 0s - loss: 0.3527     \n",
      "Epoch 10/300\n",
      "341/341 [==============================] - 0s - loss: 0.3219     \n",
      "Epoch 11/300\n",
      "341/341 [==============================] - 0s - loss: 0.2961     \n",
      "Epoch 12/300\n",
      "341/341 [==============================] - 0s - loss: 0.2747     \n",
      "Epoch 13/300\n",
      "341/341 [==============================] - 0s - loss: 0.2570     \n",
      "Epoch 14/300\n",
      "341/341 [==============================] - 0s - loss: 0.2423     \n",
      "Epoch 15/300\n",
      "341/341 [==============================] - 0s - loss: 0.2295     \n",
      "Epoch 16/300\n",
      "341/341 [==============================] - 0s - loss: 0.2188     \n",
      "Epoch 17/300\n",
      "341/341 [==============================] - 0s - loss: 0.2100     \n",
      "Epoch 18/300\n",
      "341/341 [==============================] - 0s - loss: 0.2031     \n",
      "Epoch 19/300\n",
      "341/341 [==============================] - 0s - loss: 0.1971     \n",
      "Epoch 20/300\n",
      "341/341 [==============================] - 0s - loss: 0.1916     \n",
      "Epoch 21/300\n",
      "341/341 [==============================] - 0s - loss: 0.1878     \n",
      "Epoch 22/300\n",
      "341/341 [==============================] - 0s - loss: 0.1838     \n",
      "Epoch 23/300\n",
      "341/341 [==============================] - 0s - loss: 0.1813     \n",
      "Epoch 24/300\n",
      "341/341 [==============================] - 0s - loss: 0.1782     \n",
      "Epoch 25/300\n",
      "341/341 [==============================] - 0s - loss: 0.1760     \n",
      "Epoch 26/300\n",
      "341/341 [==============================] - 0s - loss: 0.1740     \n",
      "Epoch 27/300\n",
      "341/341 [==============================] - 0s - loss: 0.1719     \n",
      "Epoch 28/300\n",
      "341/341 [==============================] - 0s - loss: 0.1698     \n",
      "Epoch 29/300\n",
      "341/341 [==============================] - 0s - loss: 0.1681     \n",
      "Epoch 30/300\n",
      "341/341 [==============================] - 0s - loss: 0.1662     \n",
      "Epoch 31/300\n",
      "341/341 [==============================] - 0s - loss: 0.1649     \n",
      "Epoch 32/300\n",
      "341/341 [==============================] - 0s - loss: 0.1634     \n",
      "Epoch 33/300\n",
      "341/341 [==============================] - 0s - loss: 0.1618     \n",
      "Epoch 34/300\n",
      "341/341 [==============================] - 0s - loss: 0.1604     \n",
      "Epoch 35/300\n",
      "341/341 [==============================] - 0s - loss: 0.1585     \n",
      "Epoch 36/300\n",
      "341/341 [==============================] - 0s - loss: 0.1577     \n",
      "Epoch 37/300\n",
      "341/341 [==============================] - 0s - loss: 0.1564     \n",
      "Epoch 38/300\n",
      "341/341 [==============================] - 0s - loss: 0.1553     \n",
      "Epoch 39/300\n",
      "341/341 [==============================] - 0s - loss: 0.1535     \n",
      "Epoch 40/300\n",
      "341/341 [==============================] - 0s - loss: 0.1530     \n",
      "Epoch 41/300\n",
      "341/341 [==============================] - 0s - loss: 0.1518     \n",
      "Epoch 42/300\n",
      "341/341 [==============================] - 0s - loss: 0.1507     \n",
      "Epoch 43/300\n",
      "341/341 [==============================] - 0s - loss: 0.1496     \n",
      "Epoch 44/300\n",
      "341/341 [==============================] - 0s - loss: 0.1488     \n",
      "Epoch 45/300\n",
      "341/341 [==============================] - 0s - loss: 0.1476     \n",
      "Epoch 46/300\n",
      "341/341 [==============================] - 0s - loss: 0.1469     \n",
      "Epoch 47/300\n",
      "341/341 [==============================] - 0s - loss: 0.1454     \n",
      "Epoch 48/300\n",
      "341/341 [==============================] - 0s - loss: 0.1448     \n",
      "Epoch 49/300\n",
      "341/341 [==============================] - 0s - loss: 0.1437     \n",
      "Epoch 50/300\n",
      "341/341 [==============================] - 0s - loss: 0.1428     \n",
      "Epoch 51/300\n",
      "341/341 [==============================] - 0s - loss: 0.1419     \n",
      "Epoch 52/300\n",
      "341/341 [==============================] - 0s - loss: 0.1413     \n",
      "Epoch 53/300\n",
      "341/341 [==============================] - 0s - loss: 0.1398     \n",
      "Epoch 54/300\n",
      "341/341 [==============================] - 0s - loss: 0.1395     \n",
      "Epoch 55/300\n",
      "341/341 [==============================] - 0s - loss: 0.1389     \n",
      "Epoch 56/300\n",
      "341/341 [==============================] - 0s - loss: 0.1371     \n",
      "Epoch 57/300\n",
      "341/341 [==============================] - 0s - loss: 0.1366     \n",
      "Epoch 58/300\n",
      "341/341 [==============================] - 0s - loss: 0.1364     \n",
      "Epoch 59/300\n",
      "341/341 [==============================] - 0s - loss: 0.1353     \n",
      "Epoch 60/300\n",
      "341/341 [==============================] - 0s - loss: 0.1343     \n",
      "Epoch 61/300\n",
      "341/341 [==============================] - 0s - loss: 0.1338     \n",
      "Epoch 62/300\n",
      "341/341 [==============================] - 0s - loss: 0.1334     \n",
      "Epoch 63/300\n",
      "341/341 [==============================] - 0s - loss: 0.1323     \n",
      "Epoch 64/300\n",
      "341/341 [==============================] - 0s - loss: 0.1316     \n",
      "Epoch 65/300\n",
      "341/341 [==============================] - 0s - loss: 0.1311     \n",
      "Epoch 66/300\n",
      "341/341 [==============================] - 0s - loss: 0.1307     \n",
      "Epoch 67/300\n",
      "341/341 [==============================] - 0s - loss: 0.1299     \n",
      "Epoch 68/300\n",
      "341/341 [==============================] - 0s - loss: 0.1293     \n",
      "Epoch 69/300\n",
      "341/341 [==============================] - 0s - loss: 0.1283     \n",
      "Epoch 70/300\n",
      "341/341 [==============================] - 0s - loss: 0.1279     \n",
      "Epoch 71/300\n",
      "341/341 [==============================] - 0s - loss: 0.1278     \n",
      "Epoch 72/300\n",
      "341/341 [==============================] - 0s - loss: 0.1272     \n",
      "Epoch 73/300\n",
      "341/341 [==============================] - 0s - loss: 0.1262     \n",
      "Epoch 74/300\n",
      "341/341 [==============================] - 0s - loss: 0.1253     \n",
      "Epoch 75/300\n",
      "341/341 [==============================] - 0s - loss: 0.1248     \n",
      "Epoch 76/300\n",
      "341/341 [==============================] - 0s - loss: 0.1238     \n",
      "Epoch 77/300\n",
      "341/341 [==============================] - 0s - loss: 0.1239     \n",
      "Epoch 78/300\n",
      "341/341 [==============================] - 0s - loss: 0.1232     \n",
      "Epoch 79/300\n",
      "341/341 [==============================] - 0s - loss: 0.1225     \n",
      "Epoch 80/300\n",
      "341/341 [==============================] - 0s - loss: 0.1217     \n",
      "Epoch 81/300\n",
      "341/341 [==============================] - 0s - loss: 0.1214     \n",
      "Epoch 82/300\n",
      "341/341 [==============================] - 0s - loss: 0.1210     \n",
      "Epoch 83/300\n",
      "341/341 [==============================] - 0s - loss: 0.1204     \n",
      "Epoch 84/300\n",
      "341/341 [==============================] - 0s - loss: 0.1200     \n",
      "Epoch 85/300\n",
      "341/341 [==============================] - 0s - loss: 0.1195     \n",
      "Epoch 86/300\n",
      "341/341 [==============================] - 0s - loss: 0.1191     \n",
      "Epoch 87/300\n",
      "341/341 [==============================] - 0s - loss: 0.1181     \n",
      "Epoch 88/300\n",
      "341/341 [==============================] - 0s - loss: 0.1181     \n",
      "Epoch 89/300\n",
      "341/341 [==============================] - 0s - loss: 0.1170     \n",
      "Epoch 90/300\n",
      "341/341 [==============================] - 0s - loss: 0.1170     \n",
      "Epoch 91/300\n",
      "341/341 [==============================] - 0s - loss: 0.1162     \n",
      "Epoch 92/300\n",
      "341/341 [==============================] - 0s - loss: 0.1160     \n",
      "Epoch 93/300\n",
      "341/341 [==============================] - 0s - loss: 0.1156     \n",
      "Epoch 94/300\n",
      "341/341 [==============================] - 0s - loss: 0.1154     \n",
      "Epoch 95/300\n",
      "341/341 [==============================] - 0s - loss: 0.1150     \n",
      "Epoch 96/300\n",
      "341/341 [==============================] - 0s - loss: 0.1141     \n",
      "Epoch 97/300\n",
      "341/341 [==============================] - 0s - loss: 0.1134     \n",
      "Epoch 98/300\n",
      "341/341 [==============================] - 0s - loss: 0.1140     \n",
      "Epoch 99/300\n",
      "341/341 [==============================] - 0s - loss: 0.1131     \n",
      "Epoch 100/300\n",
      "341/341 [==============================] - 0s - loss: 0.1123     \n",
      "Epoch 101/300\n",
      "341/341 [==============================] - 0s - loss: 0.1117     \n",
      "Epoch 102/300\n",
      "341/341 [==============================] - 0s - loss: 0.1120     \n",
      "Epoch 103/300\n",
      "341/341 [==============================] - 0s - loss: 0.1112     \n",
      "Epoch 104/300\n",
      "341/341 [==============================] - 0s - loss: 0.1112     \n",
      "Epoch 105/300\n",
      "341/341 [==============================] - 0s - loss: 0.1105     \n",
      "Epoch 106/300\n",
      "341/341 [==============================] - 0s - loss: 0.1104     \n",
      "Epoch 107/300\n",
      "341/341 [==============================] - 0s - loss: 0.1099     \n",
      "Epoch 108/300\n",
      "341/341 [==============================] - 0s - loss: 0.1092     \n",
      "Epoch 109/300\n",
      "341/341 [==============================] - 0s - loss: 0.1087     \n",
      "Epoch 110/300\n",
      "341/341 [==============================] - 0s - loss: 0.1086     \n",
      "Epoch 111/300\n",
      "341/341 [==============================] - 0s - loss: 0.1082     \n",
      "Epoch 112/300\n",
      "341/341 [==============================] - 0s - loss: 0.1072     \n",
      "Epoch 113/300\n",
      "341/341 [==============================] - 0s - loss: 0.1073     \n",
      "Epoch 114/300\n",
      "341/341 [==============================] - 0s - loss: 0.1069     \n",
      "Epoch 115/300\n",
      "341/341 [==============================] - 0s - loss: 0.1063     \n",
      "Epoch 116/300\n",
      "341/341 [==============================] - 0s - loss: 0.1063     \n",
      "Epoch 117/300\n",
      "341/341 [==============================] - 0s - loss: 0.1059     \n",
      "Epoch 118/300\n",
      "341/341 [==============================] - 0s - loss: 0.1057     \n",
      "Epoch 119/300\n",
      "341/341 [==============================] - 0s - loss: 0.1049     \n",
      "Epoch 120/300\n",
      "341/341 [==============================] - 0s - loss: 0.1050     \n",
      "Epoch 121/300\n",
      "341/341 [==============================] - 0s - loss: 0.1046     \n",
      "Epoch 122/300\n",
      "341/341 [==============================] - 0s - loss: 0.1043     \n",
      "Epoch 123/300\n",
      "341/341 [==============================] - 0s - loss: 0.1038     \n",
      "Epoch 124/300\n",
      "341/341 [==============================] - 0s - loss: 0.1033     \n",
      "Epoch 125/300\n",
      "341/341 [==============================] - 0s - loss: 0.1032     \n",
      "Epoch 126/300\n",
      "341/341 [==============================] - 0s - loss: 0.1026     \n",
      "Epoch 127/300\n",
      "341/341 [==============================] - 0s - loss: 0.1027     \n",
      "Epoch 128/300\n",
      "341/341 [==============================] - 0s - loss: 0.1022     \n",
      "Epoch 129/300\n",
      "341/341 [==============================] - 0s - loss: 0.1017     \n",
      "Epoch 130/300\n",
      "341/341 [==============================] - 0s - loss: 0.1015     \n",
      "Epoch 131/300\n",
      "341/341 [==============================] - 0s - loss: 0.1010     \n",
      "Epoch 132/300\n",
      "341/341 [==============================] - 0s - loss: 0.1005     \n",
      "Epoch 133/300\n",
      "341/341 [==============================] - 0s - loss: 0.1004     \n",
      "Epoch 134/300\n",
      "341/341 [==============================] - 0s - loss: 0.1002     \n",
      "Epoch 135/300\n",
      "341/341 [==============================] - 0s - loss: 0.1000     \n",
      "Epoch 136/300\n",
      "341/341 [==============================] - 0s - loss: 0.0998     \n",
      "Epoch 137/300\n",
      "341/341 [==============================] - 0s - loss: 0.0993     \n",
      "Epoch 138/300\n",
      "341/341 [==============================] - 0s - loss: 0.0990     \n",
      "Epoch 139/300\n",
      "341/341 [==============================] - 0s - loss: 0.0991     \n",
      "Epoch 140/300\n",
      "341/341 [==============================] - 0s - loss: 0.0980     \n",
      "Epoch 141/300\n",
      "341/341 [==============================] - 0s - loss: 0.0978     \n",
      "Epoch 142/300\n",
      "341/341 [==============================] - 0s - loss: 0.0979     \n",
      "Epoch 143/300\n",
      "341/341 [==============================] - 0s - loss: 0.0971     \n",
      "Epoch 144/300\n",
      "341/341 [==============================] - 0s - loss: 0.0968     \n",
      "Epoch 145/300\n",
      "341/341 [==============================] - 0s - loss: 0.0971     \n",
      "Epoch 146/300\n",
      "341/341 [==============================] - 0s - loss: 0.0963     \n",
      "Epoch 147/300\n",
      "341/341 [==============================] - 0s - loss: 0.0965     \n",
      "Epoch 148/300\n",
      "341/341 [==============================] - 0s - loss: 0.0955     \n",
      "Epoch 149/300\n",
      "341/341 [==============================] - 0s - loss: 0.0958     \n",
      "Epoch 150/300\n",
      "341/341 [==============================] - 0s - loss: 0.0950     \n",
      "Epoch 151/300\n",
      "341/341 [==============================] - 0s - loss: 0.0954     \n",
      "Epoch 152/300\n",
      "341/341 [==============================] - 0s - loss: 0.0949     \n",
      "Epoch 153/300\n",
      "341/341 [==============================] - 0s - loss: 0.0944     \n",
      "Epoch 154/300\n",
      "341/341 [==============================] - 0s - loss: 0.0947     \n",
      "Epoch 155/300\n",
      "341/341 [==============================] - 0s - loss: 0.0942     \n",
      "Epoch 156/300\n",
      "341/341 [==============================] - 0s - loss: 0.0938     \n",
      "Epoch 157/300\n",
      "341/341 [==============================] - 0s - loss: 0.0936     \n",
      "Epoch 158/300\n",
      "341/341 [==============================] - 0s - loss: 0.0933     \n",
      "Epoch 159/300\n",
      "341/341 [==============================] - 0s - loss: 0.0930     \n",
      "Epoch 160/300\n",
      "341/341 [==============================] - 0s - loss: 0.0926     \n",
      "Epoch 161/300\n",
      "341/341 [==============================] - 0s - loss: 0.0931     \n",
      "Epoch 162/300\n",
      "341/341 [==============================] - 0s - loss: 0.0924     \n",
      "Epoch 163/300\n",
      "341/341 [==============================] - 0s - loss: 0.0923     \n",
      "Epoch 164/300\n",
      "341/341 [==============================] - 0s - loss: 0.0925     \n",
      "Epoch 165/300\n",
      "341/341 [==============================] - 0s - loss: 0.0912     \n",
      "Epoch 166/300\n",
      "341/341 [==============================] - 0s - loss: 0.0913     \n",
      "Epoch 167/300\n",
      "341/341 [==============================] - 0s - loss: 0.0908     \n",
      "Epoch 168/300\n",
      "341/341 [==============================] - 0s - loss: 0.0912     \n",
      "Epoch 169/300\n",
      "341/341 [==============================] - 0s - loss: 0.0906     \n",
      "Epoch 170/300\n",
      "341/341 [==============================] - 0s - loss: 0.0905     \n",
      "Epoch 171/300\n",
      "341/341 [==============================] - 0s - loss: 0.0900     \n",
      "Epoch 172/300\n",
      "341/341 [==============================] - 0s - loss: 0.0895     \n",
      "Epoch 173/300\n",
      "341/341 [==============================] - 0s - loss: 0.0897     \n",
      "Epoch 174/300\n",
      "341/341 [==============================] - 0s - loss: 0.0896     \n",
      "Epoch 175/300\n",
      "341/341 [==============================] - 0s - loss: 0.0894     \n",
      "Epoch 176/300\n",
      "341/341 [==============================] - 0s - loss: 0.0893     \n",
      "Epoch 177/300\n",
      "341/341 [==============================] - 0s - loss: 0.0884     \n",
      "Epoch 178/300\n",
      "341/341 [==============================] - 0s - loss: 0.0886     \n",
      "Epoch 179/300\n",
      "341/341 [==============================] - 0s - loss: 0.0884     \n",
      "Epoch 180/300\n",
      "341/341 [==============================] - 0s - loss: 0.0883     \n",
      "Epoch 181/300\n",
      "341/341 [==============================] - 0s - loss: 0.0875     \n",
      "Epoch 182/300\n",
      "341/341 [==============================] - 0s - loss: 0.0880     \n",
      "Epoch 183/300\n",
      "341/341 [==============================] - 0s - loss: 0.0871     \n",
      "Epoch 184/300\n",
      "341/341 [==============================] - 0s - loss: 0.0868     \n",
      "Epoch 185/300\n",
      "341/341 [==============================] - 0s - loss: 0.0866     \n",
      "Epoch 186/300\n",
      "341/341 [==============================] - 0s - loss: 0.0868     \n",
      "Epoch 187/300\n",
      "341/341 [==============================] - 0s - loss: 0.0864     \n",
      "Epoch 188/300\n",
      "341/341 [==============================] - 0s - loss: 0.0863     \n",
      "Epoch 189/300\n",
      "341/341 [==============================] - 0s - loss: 0.0858     \n",
      "Epoch 190/300\n",
      "341/341 [==============================] - 0s - loss: 0.0862     \n",
      "Epoch 191/300\n",
      "341/341 [==============================] - 0s - loss: 0.0860     \n",
      "Epoch 192/300\n",
      "341/341 [==============================] - 0s - loss: 0.0854     \n",
      "Epoch 193/300\n",
      "341/341 [==============================] - 0s - loss: 0.0858     \n",
      "Epoch 194/300\n",
      "341/341 [==============================] - 0s - loss: 0.0853     \n",
      "Epoch 195/300\n",
      "341/341 [==============================] - 0s - loss: 0.0849     \n",
      "Epoch 196/300\n",
      "341/341 [==============================] - 0s - loss: 0.0848     \n",
      "Epoch 197/300\n",
      "341/341 [==============================] - 0s - loss: 0.0841     \n",
      "Epoch 198/300\n",
      "341/341 [==============================] - 0s - loss: 0.0843     \n",
      "Epoch 199/300\n",
      "341/341 [==============================] - 0s - loss: 0.0840     \n",
      "Epoch 200/300\n",
      "341/341 [==============================] - 0s - loss: 0.0839     \n",
      "Epoch 201/300\n",
      "341/341 [==============================] - 0s - loss: 0.0834     \n",
      "Epoch 202/300\n",
      "341/341 [==============================] - 0s - loss: 0.0837     \n",
      "Epoch 203/300\n",
      "341/341 [==============================] - 0s - loss: 0.0835     \n",
      "Epoch 204/300\n",
      "341/341 [==============================] - 0s - loss: 0.0835     \n",
      "Epoch 205/300\n",
      "341/341 [==============================] - 0s - loss: 0.0828     \n",
      "Epoch 206/300\n",
      "341/341 [==============================] - 0s - loss: 0.0829     \n",
      "Epoch 207/300\n",
      "341/341 [==============================] - 0s - loss: 0.0824     \n",
      "Epoch 208/300\n",
      "341/341 [==============================] - 0s - loss: 0.0823     \n",
      "Epoch 209/300\n",
      "341/341 [==============================] - 0s - loss: 0.0821     \n",
      "Epoch 210/300\n",
      "341/341 [==============================] - 0s - loss: 0.0818     \n",
      "Epoch 211/300\n",
      "341/341 [==============================] - 0s - loss: 0.0820     \n",
      "Epoch 212/300\n",
      "341/341 [==============================] - 0s - loss: 0.0818     \n",
      "Epoch 213/300\n",
      "341/341 [==============================] - 0s - loss: 0.0814     \n",
      "Epoch 214/300\n",
      "341/341 [==============================] - 0s - loss: 0.0812     \n",
      "Epoch 215/300\n",
      "341/341 [==============================] - 0s - loss: 0.0811     \n",
      "Epoch 216/300\n",
      "341/341 [==============================] - 0s - loss: 0.0807     \n",
      "Epoch 217/300\n",
      "341/341 [==============================] - 0s - loss: 0.0808     \n",
      "Epoch 218/300\n",
      "341/341 [==============================] - 0s - loss: 0.0805     \n",
      "Epoch 219/300\n",
      "341/341 [==============================] - 0s - loss: 0.0799     \n",
      "Epoch 220/300\n",
      "341/341 [==============================] - 0s - loss: 0.0801     \n",
      "Epoch 221/300\n",
      "341/341 [==============================] - 0s - loss: 0.0798     \n",
      "Epoch 222/300\n",
      "341/341 [==============================] - 0s - loss: 0.0799     \n",
      "Epoch 223/300\n",
      "341/341 [==============================] - 0s - loss: 0.0798     \n",
      "Epoch 224/300\n",
      "341/341 [==============================] - 0s - loss: 0.0794     \n",
      "Epoch 225/300\n",
      "341/341 [==============================] - 0s - loss: 0.0792     \n",
      "Epoch 226/300\n",
      "341/341 [==============================] - 0s - loss: 0.0787     \n",
      "Epoch 227/300\n",
      "341/341 [==============================] - 0s - loss: 0.0788     \n",
      "Epoch 228/300\n",
      "341/341 [==============================] - 0s - loss: 0.0787     \n",
      "Epoch 229/300\n",
      "341/341 [==============================] - 0s - loss: 0.0788     \n",
      "Epoch 230/300\n",
      "341/341 [==============================] - 0s - loss: 0.0783     \n",
      "Epoch 231/300\n",
      "341/341 [==============================] - 0s - loss: 0.0780     \n",
      "Epoch 232/300\n",
      "341/341 [==============================] - 0s - loss: 0.0780     \n",
      "Epoch 233/300\n",
      "341/341 [==============================] - 0s - loss: 0.0779     \n",
      "Epoch 234/300\n",
      "341/341 [==============================] - 0s - loss: 0.0778     \n",
      "Epoch 235/300\n",
      "341/341 [==============================] - 0s - loss: 0.0776     \n",
      "Epoch 236/300\n",
      "341/341 [==============================] - 0s - loss: 0.0777     \n",
      "Epoch 237/300\n",
      "341/341 [==============================] - 0s - loss: 0.0772     \n",
      "Epoch 238/300\n",
      "341/341 [==============================] - 0s - loss: 0.0771     \n",
      "Epoch 239/300\n",
      "341/341 [==============================] - 0s - loss: 0.0770     \n",
      "Epoch 240/300\n",
      "341/341 [==============================] - 0s - loss: 0.0764     \n",
      "Epoch 241/300\n",
      "341/341 [==============================] - 0s - loss: 0.0765     \n",
      "Epoch 242/300\n",
      "341/341 [==============================] - 0s - loss: 0.0763     \n",
      "Epoch 243/300\n",
      "341/341 [==============================] - 0s - loss: 0.0757     \n",
      "Epoch 244/300\n",
      "341/341 [==============================] - 0s - loss: 0.0757     \n",
      "Epoch 245/300\n",
      "341/341 [==============================] - 0s - loss: 0.0757     \n",
      "Epoch 246/300\n",
      "341/341 [==============================] - 0s - loss: 0.0755     \n",
      "Epoch 247/300\n",
      "341/341 [==============================] - 0s - loss: 0.0755     \n",
      "Epoch 248/300\n",
      "341/341 [==============================] - 0s - loss: 0.0753     \n",
      "Epoch 249/300\n",
      "341/341 [==============================] - 0s - loss: 0.0750     \n",
      "Epoch 250/300\n",
      "341/341 [==============================] - 0s - loss: 0.0755     \n",
      "Epoch 251/300\n",
      "341/341 [==============================] - 0s - loss: 0.0748     \n",
      "Epoch 252/300\n",
      "341/341 [==============================] - 0s - loss: 0.0751     \n",
      "Epoch 253/300\n",
      "341/341 [==============================] - 0s - loss: 0.0745     \n",
      "Epoch 254/300\n",
      "341/341 [==============================] - 0s - loss: 0.0743     \n",
      "Epoch 255/300\n",
      "341/341 [==============================] - 0s - loss: 0.0738     \n",
      "Epoch 256/300\n",
      "341/341 [==============================] - 0s - loss: 0.0740     \n",
      "Epoch 257/300\n",
      "341/341 [==============================] - 0s - loss: 0.0738     \n",
      "Epoch 258/300\n",
      "341/341 [==============================] - 0s - loss: 0.0738     \n",
      "Epoch 259/300\n",
      "341/341 [==============================] - 0s - loss: 0.0733     \n",
      "Epoch 260/300\n",
      "341/341 [==============================] - 0s - loss: 0.0734     \n",
      "Epoch 261/300\n",
      "341/341 [==============================] - 0s - loss: 0.0730     \n",
      "Epoch 262/300\n",
      "341/341 [==============================] - 0s - loss: 0.0730     \n",
      "Epoch 263/300\n",
      "341/341 [==============================] - 0s - loss: 0.0732     \n",
      "Epoch 264/300\n",
      "341/341 [==============================] - 0s - loss: 0.0727     \n",
      "Epoch 265/300\n",
      "341/341 [==============================] - 0s - loss: 0.0722     \n",
      "Epoch 266/300\n",
      "341/341 [==============================] - 0s - loss: 0.0728     \n",
      "Epoch 267/300\n",
      "341/341 [==============================] - 0s - loss: 0.0721     \n",
      "Epoch 268/300\n",
      "341/341 [==============================] - 0s - loss: 0.0722     \n",
      "Epoch 269/300\n",
      "341/341 [==============================] - 0s - loss: 0.0722     \n",
      "Epoch 270/300\n",
      "341/341 [==============================] - 0s - loss: 0.0717     \n",
      "Epoch 271/300\n",
      "341/341 [==============================] - 0s - loss: 0.0718     \n",
      "Epoch 272/300\n",
      "341/341 [==============================] - 0s - loss: 0.0713     \n",
      "Epoch 273/300\n",
      "341/341 [==============================] - 0s - loss: 0.0713     \n",
      "Epoch 274/300\n",
      "341/341 [==============================] - 0s - loss: 0.0712     \n",
      "Epoch 275/300\n",
      "341/341 [==============================] - 0s - loss: 0.0710     \n",
      "Epoch 276/300\n",
      "341/341 [==============================] - 0s - loss: 0.0706     \n",
      "Epoch 277/300\n",
      "341/341 [==============================] - 0s - loss: 0.0708     \n",
      "Epoch 278/300\n",
      "341/341 [==============================] - 0s - loss: 0.0706     \n",
      "Epoch 279/300\n",
      "341/341 [==============================] - 0s - loss: 0.0703     \n",
      "Epoch 280/300\n",
      "341/341 [==============================] - 0s - loss: 0.0702     \n",
      "Epoch 281/300\n",
      "341/341 [==============================] - 0s - loss: 0.0703     \n",
      "Epoch 282/300\n",
      "341/341 [==============================] - 0s - loss: 0.0701     \n",
      "Epoch 283/300\n",
      "341/341 [==============================] - 0s - loss: 0.0698     \n",
      "Epoch 284/300\n",
      "341/341 [==============================] - 0s - loss: 0.0700     \n",
      "Epoch 285/300\n",
      "341/341 [==============================] - 0s - loss: 0.0697     \n",
      "Epoch 286/300\n",
      "341/341 [==============================] - 0s - loss: 0.0695     \n",
      "Epoch 287/300\n",
      "341/341 [==============================] - 0s - loss: 0.0693     \n",
      "Epoch 288/300\n",
      "341/341 [==============================] - 0s - loss: 0.0690     \n",
      "Epoch 289/300\n",
      "341/341 [==============================] - 0s - loss: 0.0688     \n",
      "Epoch 290/300\n",
      "341/341 [==============================] - 0s - loss: 0.0689     \n",
      "Epoch 291/300\n",
      "341/341 [==============================] - 0s - loss: 0.0687     \n",
      "Epoch 292/300\n",
      "341/341 [==============================] - 0s - loss: 0.0687     \n",
      "Epoch 293/300\n",
      "341/341 [==============================] - 0s - loss: 0.0685     \n",
      "Epoch 294/300\n",
      "341/341 [==============================] - 0s - loss: 0.0685     \n",
      "Epoch 295/300\n",
      "341/341 [==============================] - 0s - loss: 0.0683     \n",
      "Epoch 296/300\n",
      "341/341 [==============================] - 0s - loss: 0.0685     \n",
      "Epoch 297/300\n",
      "341/341 [==============================] - 0s - loss: 0.0682     \n",
      "Epoch 298/300\n",
      "341/341 [==============================] - 0s - loss: 0.0681     \n",
      "Epoch 299/300\n",
      "341/341 [==============================] - 0s - loss: 0.0676     \n",
      "Epoch 300/300\n",
      "341/341 [==============================] - 0s - loss: 0.0679     \n",
      "32/38 [========================>.....] - ETA: 0sEpoch 1/300\n",
      "341/341 [==============================] - 0s - loss: 1.0126     \n",
      "Epoch 2/300\n",
      "341/341 [==============================] - 0s - loss: 0.8939     \n",
      "Epoch 3/300\n",
      "341/341 [==============================] - 0s - loss: 0.7819     \n",
      "Epoch 4/300\n",
      "341/341 [==============================] - 0s - loss: 0.6787     \n",
      "Epoch 5/300\n",
      "341/341 [==============================] - 0s - loss: 0.5855     \n",
      "Epoch 6/300\n",
      "341/341 [==============================] - 0s - loss: 0.5055     \n",
      "Epoch 7/300\n",
      "341/341 [==============================] - 0s - loss: 0.4420     \n",
      "Epoch 8/300\n",
      "341/341 [==============================] - 0s - loss: 0.3926     \n",
      "Epoch 9/300\n",
      "341/341 [==============================] - 0s - loss: 0.3546     \n",
      "Epoch 10/300\n",
      "341/341 [==============================] - 0s - loss: 0.3214     \n",
      "Epoch 11/300\n",
      "341/341 [==============================] - 0s - loss: 0.2951     \n",
      "Epoch 12/300\n",
      "341/341 [==============================] - 0s - loss: 0.2738     \n",
      "Epoch 13/300\n",
      "341/341 [==============================] - 0s - loss: 0.2556     \n",
      "Epoch 14/300\n",
      "341/341 [==============================] - 0s - loss: 0.2402     \n",
      "Epoch 15/300\n",
      "341/341 [==============================] - 0s - loss: 0.2266     \n",
      "Epoch 16/300\n",
      "341/341 [==============================] - 0s - loss: 0.2159     \n",
      "Epoch 17/300\n",
      "341/341 [==============================] - 0s - loss: 0.2065     \n",
      "Epoch 18/300\n",
      "341/341 [==============================] - 0s - loss: 0.1984     \n",
      "Epoch 19/300\n",
      "341/341 [==============================] - 0s - loss: 0.1922     \n",
      "Epoch 20/300\n",
      "341/341 [==============================] - 0s - loss: 0.1860     \n",
      "Epoch 21/300\n",
      "341/341 [==============================] - 0s - loss: 0.1810     \n",
      "Epoch 22/300\n",
      "341/341 [==============================] - 0s - loss: 0.1764     \n",
      "Epoch 23/300\n",
      "341/341 [==============================] - 0s - loss: 0.1733     \n",
      "Epoch 24/300\n",
      "341/341 [==============================] - 0s - loss: 0.1695     \n",
      "Epoch 25/300\n",
      "341/341 [==============================] - 0s - loss: 0.1667     \n",
      "Epoch 26/300\n",
      "341/341 [==============================] - 0s - loss: 0.1638     \n",
      "Epoch 27/300\n",
      "341/341 [==============================] - 0s - loss: 0.1614     \n",
      "Epoch 28/300\n",
      "341/341 [==============================] - 0s - loss: 0.1592     \n",
      "Epoch 29/300\n",
      "341/341 [==============================] - 0s - loss: 0.1569     \n",
      "Epoch 30/300\n",
      "341/341 [==============================] - 0s - loss: 0.1547     \n",
      "Epoch 31/300\n",
      "341/341 [==============================] - 0s - loss: 0.1532     \n",
      "Epoch 32/300\n",
      "341/341 [==============================] - 0s - loss: 0.1513     \n",
      "Epoch 33/300\n",
      "341/341 [==============================] - 0s - loss: 0.1496     \n",
      "Epoch 34/300\n",
      "341/341 [==============================] - 0s - loss: 0.1480     \n",
      "Epoch 35/300\n",
      "341/341 [==============================] - 0s - loss: 0.1464     \n",
      "Epoch 36/300\n",
      "341/341 [==============================] - 0s - loss: 0.1452     \n",
      "Epoch 37/300\n",
      "341/341 [==============================] - 0s - loss: 0.1440     \n",
      "Epoch 38/300\n",
      "341/341 [==============================] - 0s - loss: 0.1425     \n",
      "Epoch 39/300\n",
      "341/341 [==============================] - 0s - loss: 0.1409     \n",
      "Epoch 40/300\n",
      "341/341 [==============================] - 0s - loss: 0.1399     \n",
      "Epoch 41/300\n",
      "341/341 [==============================] - 0s - loss: 0.1387     \n",
      "Epoch 42/300\n",
      "341/341 [==============================] - 0s - loss: 0.1375     \n",
      "Epoch 43/300\n",
      "341/341 [==============================] - 0s - loss: 0.1365     \n",
      "Epoch 44/300\n",
      "341/341 [==============================] - 0s - loss: 0.1352     \n",
      "Epoch 45/300\n",
      "341/341 [==============================] - 0s - loss: 0.1345     \n",
      "Epoch 46/300\n",
      "341/341 [==============================] - 0s - loss: 0.1335     \n",
      "Epoch 47/300\n",
      "341/341 [==============================] - 0s - loss: 0.1327     \n",
      "Epoch 48/300\n",
      "341/341 [==============================] - 0s - loss: 0.1317     \n",
      "Epoch 49/300\n",
      "341/341 [==============================] - 0s - loss: 0.1306     \n",
      "Epoch 50/300\n",
      "341/341 [==============================] - 0s - loss: 0.1297     \n",
      "Epoch 51/300\n",
      "341/341 [==============================] - 0s - loss: 0.1290     \n",
      "Epoch 52/300\n",
      "341/341 [==============================] - 0s - loss: 0.1280     \n",
      "Epoch 53/300\n",
      "341/341 [==============================] - 0s - loss: 0.1274     \n",
      "Epoch 54/300\n",
      "341/341 [==============================] - 0s - loss: 0.1266     \n",
      "Epoch 55/300\n",
      "341/341 [==============================] - 0s - loss: 0.1258     \n",
      "Epoch 56/300\n",
      "341/341 [==============================] - 0s - loss: 0.1251     \n",
      "Epoch 57/300\n",
      "341/341 [==============================] - 0s - loss: 0.1244     \n",
      "Epoch 58/300\n",
      "341/341 [==============================] - 0s - loss: 0.1240     \n",
      "Epoch 59/300\n",
      "341/341 [==============================] - 0s - loss: 0.1232     \n",
      "Epoch 60/300\n",
      "341/341 [==============================] - 0s - loss: 0.1223     \n",
      "Epoch 61/300\n",
      "341/341 [==============================] - 0s - loss: 0.1216     \n",
      "Epoch 62/300\n",
      "341/341 [==============================] - 0s - loss: 0.1211     \n",
      "Epoch 63/300\n",
      "341/341 [==============================] - 0s - loss: 0.1203     \n",
      "Epoch 64/300\n",
      "341/341 [==============================] - 0s - loss: 0.1199     \n",
      "Epoch 65/300\n",
      "341/341 [==============================] - 0s - loss: 0.1194     \n",
      "Epoch 66/300\n",
      "341/341 [==============================] - 0s - loss: 0.1187     \n",
      "Epoch 67/300\n",
      "341/341 [==============================] - 0s - loss: 0.1182     \n",
      "Epoch 68/300\n",
      "341/341 [==============================] - 0s - loss: 0.1175     \n",
      "Epoch 69/300\n",
      "341/341 [==============================] - 0s - loss: 0.1166     \n",
      "Epoch 70/300\n",
      "341/341 [==============================] - 0s - loss: 0.1164     \n",
      "Epoch 71/300\n",
      "341/341 [==============================] - 0s - loss: 0.1156     \n",
      "Epoch 72/300\n",
      "341/341 [==============================] - 0s - loss: 0.1150     \n",
      "Epoch 73/300\n",
      "341/341 [==============================] - 0s - loss: 0.1144     \n",
      "Epoch 74/300\n",
      "341/341 [==============================] - 0s - loss: 0.1141     \n",
      "Epoch 75/300\n",
      "341/341 [==============================] - 0s - loss: 0.1135     \n",
      "Epoch 76/300\n",
      "341/341 [==============================] - 0s - loss: 0.1130     \n",
      "Epoch 77/300\n",
      "341/341 [==============================] - 0s - loss: 0.1126     \n",
      "Epoch 78/300\n",
      "341/341 [==============================] - 0s - loss: 0.1120     \n",
      "Epoch 79/300\n",
      "341/341 [==============================] - 0s - loss: 0.1116     \n",
      "Epoch 80/300\n",
      "341/341 [==============================] - 0s - loss: 0.1113     \n",
      "Epoch 81/300\n",
      "341/341 [==============================] - 0s - loss: 0.1109     \n",
      "Epoch 82/300\n",
      "341/341 [==============================] - 0s - loss: 0.1101     \n",
      "Epoch 83/300\n",
      "341/341 [==============================] - 0s - loss: 0.1096     \n",
      "Epoch 84/300\n",
      "341/341 [==============================] - 0s - loss: 0.1091     \n",
      "Epoch 85/300\n",
      "341/341 [==============================] - 0s - loss: 0.1089     \n",
      "Epoch 86/300\n",
      "341/341 [==============================] - 0s - loss: 0.1084     \n",
      "Epoch 87/300\n",
      "341/341 [==============================] - 0s - loss: 0.1082     \n",
      "Epoch 88/300\n",
      "341/341 [==============================] - 0s - loss: 0.1076     \n",
      "Epoch 89/300\n",
      "341/341 [==============================] - 0s - loss: 0.1070     \n",
      "Epoch 90/300\n",
      "341/341 [==============================] - 0s - loss: 0.1066     \n",
      "Epoch 91/300\n",
      "341/341 [==============================] - 0s - loss: 0.1063     \n",
      "Epoch 92/300\n",
      "341/341 [==============================] - 0s - loss: 0.1057     \n",
      "Epoch 93/300\n",
      "341/341 [==============================] - 0s - loss: 0.1053     \n",
      "Epoch 94/300\n",
      "341/341 [==============================] - 0s - loss: 0.1047     \n",
      "Epoch 95/300\n",
      "341/341 [==============================] - 0s - loss: 0.1043     \n",
      "Epoch 96/300\n",
      "341/341 [==============================] - 0s - loss: 0.1042     \n",
      "Epoch 97/300\n",
      "341/341 [==============================] - 0s - loss: 0.1039     \n",
      "Epoch 98/300\n",
      "341/341 [==============================] - 0s - loss: 0.1035     \n",
      "Epoch 99/300\n",
      "341/341 [==============================] - 0s - loss: 0.1029     \n",
      "Epoch 100/300\n",
      "341/341 [==============================] - 0s - loss: 0.1024     \n",
      "Epoch 101/300\n",
      "341/341 [==============================] - 0s - loss: 0.1022     \n",
      "Epoch 102/300\n",
      "341/341 [==============================] - 0s - loss: 0.1016     \n",
      "Epoch 103/300\n",
      "341/341 [==============================] - 0s - loss: 0.1016     \n",
      "Epoch 104/300\n",
      "341/341 [==============================] - 0s - loss: 0.1012     \n",
      "Epoch 105/300\n",
      "341/341 [==============================] - 0s - loss: 0.1007     \n",
      "Epoch 106/300\n",
      "341/341 [==============================] - 0s - loss: 0.1005     \n",
      "Epoch 107/300\n",
      "341/341 [==============================] - 0s - loss: 0.0999     \n",
      "Epoch 108/300\n",
      "341/341 [==============================] - 0s - loss: 0.0997     \n",
      "Epoch 109/300\n",
      "341/341 [==============================] - 0s - loss: 0.0994     \n",
      "Epoch 110/300\n",
      "341/341 [==============================] - 0s - loss: 0.0991     \n",
      "Epoch 111/300\n",
      "341/341 [==============================] - 0s - loss: 0.0988     \n",
      "Epoch 112/300\n",
      "341/341 [==============================] - 0s - loss: 0.0986     \n",
      "Epoch 113/300\n",
      "341/341 [==============================] - 0s - loss: 0.0982     \n",
      "Epoch 114/300\n",
      "341/341 [==============================] - 0s - loss: 0.0978     \n",
      "Epoch 115/300\n",
      "341/341 [==============================] - 0s - loss: 0.0973     \n",
      "Epoch 116/300\n",
      "341/341 [==============================] - 0s - loss: 0.0970     \n",
      "Epoch 117/300\n",
      "341/341 [==============================] - 0s - loss: 0.0968     \n",
      "Epoch 118/300\n",
      "341/341 [==============================] - 0s - loss: 0.0966     \n",
      "Epoch 119/300\n",
      "341/341 [==============================] - 0s - loss: 0.0962     \n",
      "Epoch 120/300\n",
      "341/341 [==============================] - 0s - loss: 0.0960     \n",
      "Epoch 121/300\n",
      "341/341 [==============================] - 0s - loss: 0.0956     \n",
      "Epoch 122/300\n",
      "341/341 [==============================] - 0s - loss: 0.0952     \n",
      "Epoch 123/300\n",
      "341/341 [==============================] - 0s - loss: 0.0951     \n",
      "Epoch 124/300\n",
      "341/341 [==============================] - 0s - loss: 0.0948     \n",
      "Epoch 125/300\n",
      "341/341 [==============================] - 0s - loss: 0.0945     \n",
      "Epoch 126/300\n",
      "341/341 [==============================] - 0s - loss: 0.0940     \n",
      "Epoch 127/300\n",
      "341/341 [==============================] - 0s - loss: 0.0939     \n",
      "Epoch 128/300\n",
      "341/341 [==============================] - 0s - loss: 0.0937     \n",
      "Epoch 129/300\n",
      "341/341 [==============================] - 0s - loss: 0.0933     \n",
      "Epoch 130/300\n",
      "341/341 [==============================] - 0s - loss: 0.0929     \n",
      "Epoch 131/300\n",
      "341/341 [==============================] - 0s - loss: 0.0927     \n",
      "Epoch 132/300\n",
      "341/341 [==============================] - 0s - loss: 0.0925     \n",
      "Epoch 133/300\n",
      "341/341 [==============================] - 0s - loss: 0.0921     \n",
      "Epoch 134/300\n",
      "341/341 [==============================] - 0s - loss: 0.0920     \n",
      "Epoch 135/300\n",
      "341/341 [==============================] - 0s - loss: 0.0916     \n",
      "Epoch 136/300\n",
      "341/341 [==============================] - 0s - loss: 0.0916     \n",
      "Epoch 137/300\n",
      "341/341 [==============================] - 0s - loss: 0.0912     \n",
      "Epoch 138/300\n",
      "341/341 [==============================] - 0s - loss: 0.0909     \n",
      "Epoch 139/300\n",
      "341/341 [==============================] - 0s - loss: 0.0905     \n",
      "Epoch 140/300\n",
      "341/341 [==============================] - 0s - loss: 0.0905     \n",
      "Epoch 141/300\n",
      "341/341 [==============================] - 0s - loss: 0.0902     \n",
      "Epoch 142/300\n",
      "341/341 [==============================] - 0s - loss: 0.0901     \n",
      "Epoch 143/300\n",
      "341/341 [==============================] - 0s - loss: 0.0902     \n",
      "Epoch 144/300\n",
      "341/341 [==============================] - 0s - loss: 0.0892     \n",
      "Epoch 145/300\n",
      "341/341 [==============================] - 0s - loss: 0.0889     \n",
      "Epoch 146/300\n",
      "341/341 [==============================] - 0s - loss: 0.0886     \n",
      "Epoch 147/300\n",
      "341/341 [==============================] - 0s - loss: 0.0891     \n",
      "Epoch 148/300\n",
      "341/341 [==============================] - 0s - loss: 0.0883     \n",
      "Epoch 149/300\n",
      "341/341 [==============================] - 0s - loss: 0.0881     \n",
      "Epoch 150/300\n",
      "341/341 [==============================] - 0s - loss: 0.0877     \n",
      "Epoch 151/300\n",
      "341/341 [==============================] - 0s - loss: 0.0879     \n",
      "Epoch 152/300\n",
      "341/341 [==============================] - 0s - loss: 0.0873     \n",
      "Epoch 153/300\n",
      "341/341 [==============================] - 0s - loss: 0.0869     \n",
      "Epoch 154/300\n",
      "341/341 [==============================] - 0s - loss: 0.0869     \n",
      "Epoch 155/300\n",
      "341/341 [==============================] - 0s - loss: 0.0869     \n",
      "Epoch 156/300\n",
      "341/341 [==============================] - 0s - loss: 0.0866     \n",
      "Epoch 157/300\n",
      "341/341 [==============================] - 0s - loss: 0.0863     \n",
      "Epoch 158/300\n",
      "341/341 [==============================] - 0s - loss: 0.0860     \n",
      "Epoch 159/300\n",
      "341/341 [==============================] - 0s - loss: 0.0859     \n",
      "Epoch 160/300\n",
      "341/341 [==============================] - 0s - loss: 0.0852     \n",
      "Epoch 161/300\n",
      "341/341 [==============================] - 0s - loss: 0.0851     \n",
      "Epoch 162/300\n",
      "341/341 [==============================] - 0s - loss: 0.0850     \n",
      "Epoch 163/300\n",
      "341/341 [==============================] - 0s - loss: 0.0848     \n",
      "Epoch 164/300\n",
      "341/341 [==============================] - 0s - loss: 0.0846     \n",
      "Epoch 165/300\n",
      "341/341 [==============================] - 0s - loss: 0.0843     \n",
      "Epoch 166/300\n",
      "341/341 [==============================] - 0s - loss: 0.0840     \n",
      "Epoch 167/300\n",
      "341/341 [==============================] - 0s - loss: 0.0838     \n",
      "Epoch 168/300\n",
      "341/341 [==============================] - 0s - loss: 0.0835     \n",
      "Epoch 169/300\n",
      "341/341 [==============================] - 0s - loss: 0.0837     \n",
      "Epoch 170/300\n",
      "341/341 [==============================] - 0s - loss: 0.0831     \n",
      "Epoch 171/300\n",
      "341/341 [==============================] - 0s - loss: 0.0833     \n",
      "Epoch 172/300\n",
      "341/341 [==============================] - 0s - loss: 0.0828     \n",
      "Epoch 173/300\n",
      "341/341 [==============================] - 0s - loss: 0.0827     \n",
      "Epoch 174/300\n",
      "341/341 [==============================] - 0s - loss: 0.0825     \n",
      "Epoch 175/300\n",
      "341/341 [==============================] - 0s - loss: 0.0823     \n",
      "Epoch 176/300\n",
      "341/341 [==============================] - 0s - loss: 0.0822     \n",
      "Epoch 177/300\n",
      "341/341 [==============================] - 0s - loss: 0.0817     \n",
      "Epoch 178/300\n",
      "341/341 [==============================] - 0s - loss: 0.0818     \n",
      "Epoch 179/300\n",
      "341/341 [==============================] - 0s - loss: 0.0811     \n",
      "Epoch 180/300\n",
      "341/341 [==============================] - 0s - loss: 0.0812     \n",
      "Epoch 181/300\n",
      "341/341 [==============================] - 0s - loss: 0.0811     \n",
      "Epoch 182/300\n",
      "341/341 [==============================] - 0s - loss: 0.0807     \n",
      "Epoch 183/300\n",
      "341/341 [==============================] - 0s - loss: 0.0807     \n",
      "Epoch 184/300\n",
      "341/341 [==============================] - 0s - loss: 0.0802     \n",
      "Epoch 185/300\n",
      "341/341 [==============================] - 0s - loss: 0.0801     \n",
      "Epoch 186/300\n",
      "341/341 [==============================] - 0s - loss: 0.0798     \n",
      "Epoch 187/300\n",
      "341/341 [==============================] - 0s - loss: 0.0796     \n",
      "Epoch 188/300\n",
      "341/341 [==============================] - 0s - loss: 0.0796     \n",
      "Epoch 189/300\n",
      "341/341 [==============================] - 0s - loss: 0.0795     \n",
      "Epoch 190/300\n",
      "341/341 [==============================] - 0s - loss: 0.0792     \n",
      "Epoch 191/300\n",
      "341/341 [==============================] - 0s - loss: 0.0792     \n",
      "Epoch 192/300\n",
      "341/341 [==============================] - 0s - loss: 0.0786     \n",
      "Epoch 193/300\n",
      "341/341 [==============================] - 0s - loss: 0.0785     \n",
      "Epoch 194/300\n",
      "341/341 [==============================] - 0s - loss: 0.0784     \n",
      "Epoch 195/300\n",
      "341/341 [==============================] - 0s - loss: 0.0785     \n",
      "Epoch 196/300\n",
      "341/341 [==============================] - 0s - loss: 0.0779     \n",
      "Epoch 197/300\n",
      "341/341 [==============================] - 0s - loss: 0.0778     \n",
      "Epoch 198/300\n",
      "341/341 [==============================] - 0s - loss: 0.0774     \n",
      "Epoch 199/300\n",
      "341/341 [==============================] - 0s - loss: 0.0776     \n",
      "Epoch 200/300\n",
      "341/341 [==============================] - 0s - loss: 0.0775     \n",
      "Epoch 201/300\n",
      "341/341 [==============================] - 0s - loss: 0.0772     \n",
      "Epoch 202/300\n",
      "341/341 [==============================] - 0s - loss: 0.0772     \n",
      "Epoch 203/300\n",
      "341/341 [==============================] - 0s - loss: 0.0769     \n",
      "Epoch 204/300\n",
      "341/341 [==============================] - 0s - loss: 0.0766     \n",
      "Epoch 205/300\n",
      "341/341 [==============================] - 0s - loss: 0.0762     \n",
      "Epoch 206/300\n",
      "341/341 [==============================] - 0s - loss: 0.0761     \n",
      "Epoch 207/300\n",
      "341/341 [==============================] - 0s - loss: 0.0760     \n",
      "Epoch 208/300\n",
      "341/341 [==============================] - 0s - loss: 0.0760     \n",
      "Epoch 209/300\n",
      "341/341 [==============================] - 0s - loss: 0.0758     \n",
      "Epoch 210/300\n",
      "341/341 [==============================] - 0s - loss: 0.0754     \n",
      "Epoch 211/300\n",
      "341/341 [==============================] - 0s - loss: 0.0754     \n",
      "Epoch 212/300\n",
      "341/341 [==============================] - 0s - loss: 0.0752     \n",
      "Epoch 213/300\n",
      "341/341 [==============================] - 0s - loss: 0.0753     \n",
      "Epoch 214/300\n",
      "341/341 [==============================] - 0s - loss: 0.0749     \n",
      "Epoch 215/300\n",
      "341/341 [==============================] - 0s - loss: 0.0747     \n",
      "Epoch 216/300\n",
      "341/341 [==============================] - 0s - loss: 0.0746     "
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "Xm = X_train_scaled.as_matrix()\n",
    "ym = y_train_scaled.as_matrix()\n",
    "kfold = cross_validation.KFold(len(Xm), 10)\n",
    "cvscores = []\n",
    "for i, (train, val) in enumerate(kfold):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=Xm.shape[1], init='uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1, init='uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.2)\n",
    "    model.compile(optimizer='sgd',loss='mean_squared_error')\n",
    "    # Fit the model\n",
    "    model.fit(Xm[train], ym[train], nb_epoch=300)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(Xm[val], ym[val])\n",
    "    cvscores.append(scores)\n",
    "\n",
    "mse_cv = np.mean(cvscores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) La función $\\texttt{load_CIFAR10}$ permite generar el training set, testing set y validation set a partir de los datos de CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ejercicio 3\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "# Inicializar semilla aleatoria\n",
    "np.random.seed(20)\n",
    "\n",
    "# Carga de un archivo de CIFAR\n",
    "def load_CIFAR_one(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "# Carga todos los archivos CIFAR y generar Training set, Testing set y Validation set\n",
    "def load_CIFAR10(PATH, n_files=6):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    # Juntar toda la data de entrenamiento\n",
    "    for b in range(1, n_files):\n",
    "        f = os.path.join(PATH, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_one(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    # Obtener subconjunto para validacion a partir de data de entrenamiento\n",
    "    v_size = np.random.randint(1000, 5000)\n",
    "    indices = np.random.choice(np.arange(Xtr.shape[0]), v_size)\n",
    "    mask_tr = np.ones(Xtr.shape[0], dtype=bool)\n",
    "    mask_tr[indices] = False\n",
    "    mask_v = np.invert(mask_tr)\n",
    "    Xv = Xtr[mask_v]\n",
    "    Yv = Ytr[mask_v]\n",
    "    Xtr = Xtr[mask_tr]\n",
    "    Ytr = Ytr[mask_tr]\n",
    "    # Obtener data de prueba\n",
    "    Xte, Yte = load_CIFAR_one(os.path.join(PATH, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte, Xv, Yv\n",
    "\n",
    "# Cargar desde carpeta local data\n",
    "Xtr, Ytr, Xte, Yte, Xv, Yv = load_CIFAR10(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Escalamiento y centrado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/lib64/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Centrar dataset y escalar segun preferencia\n",
    "def preprocess(X, with_mean=True, with_std=True):\n",
    "    scaler = StandardScaler(with_mean, with_std).fit(X)\n",
    "    return scaler.transform(X)\n",
    "\n",
    "# Data solo centrada\n",
    "#Xtr_c = preprocess(Xtr, with_mean=True, with_std=False)\n",
    "# Data solo escalada\n",
    "#Xtr_s = preprocess(Xtr, with_mean=False, with_std=True)\n",
    "# Data centrada y escalada\n",
    "Xtr_cs = preprocess(Xtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "c) Creación de red neuronal para problema CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-236b5c203852>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mMLPmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mMLPmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mMLPmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mMLPmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    274\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m                     \u001b[0minput_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_input_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mcreate_input_layer\u001b[1;34m(self, batch_input_shape, input_dtype, name)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    486\u001b[0m                                     '`layer.build(batch_input_shape)`')\n\u001b[0;32m    487\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/keras/layers/core.pyc\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m         self.W = self.init((input_dim, self.output_dim),\n\u001b[1;32m--> 703\u001b[1;33m                            name='{}_W'.format(self.name))\n\u001b[0m\u001b[0;32m    704\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m             self.b = K.zeros((self.output_dim,),\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/keras/initializations.pyc\u001b[0m in \u001b[0;36muniform\u001b[1;34m(shape, scale, name)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_uniform_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36mrandom_uniform_variable\u001b[1;34m(shape, low, high, dtype, name)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrandom_uniform_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_FLOATX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     return variable(np.random.uniform(low=low, high=high, size=shape),\n\u001b[0m\u001b[0;32m    111\u001b[0m                     dtype=dtype, name=name)\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.uniform (numpy/random/mtrand/mtrand.c:17358)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.cont2_array_sc (numpy/random/mtrand/mtrand.c:3224)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# Modelo 1: Multilayer Perceptron\n",
    "input_dim = Xtr.shape[1:]\n",
    "MLPmodel = Sequential()\n",
    "MLPmodel.add(Dense(32, input_dim=input_dim, init='uniform'))\n",
    "MLPmodel.add(Activation('relu'))\n",
    "MLPmodel.add(Dense(10, init='uniform'))\n",
    "MLPmodel.add(Activation('softmax'))\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "MLPmodel.compile(loss='mean_squared_error',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "MLPmodel.fit(Xtr,Ytr)\n",
    "#MLPmodel.fit(Xtr_cs, Ytr, nb_epoch=20, batch_size=16)\n",
    "#score = MLPmodel.evaluate(Xte, Yte, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
